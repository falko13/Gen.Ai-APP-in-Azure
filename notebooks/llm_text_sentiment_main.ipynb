{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Script for Component Creation and Pipeline Design"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Environment\n",
        "- Install necessary libraries including `datasets` for data handling, `torch` and `torchdata` for neural network operations, and `transformers`, `evaluate`, `rouge_score`, `loralib`, and `peft` for model training, evaluation, and enhancement."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet\n",
        "\n",
        "    \n",
        "# Installing the Reinforcement Learning library directly from github.\n",
        "# %pip install git+https://github.com/lvwerra/trl.git@25fa1bd  "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: datasets==2.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.17.0)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (1.25.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (12.0.1)\nRequirement already satisfied: pyarrow-hotfix in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (4.65.0)\nRequirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\nRequirement already satisfied: multiprocess in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.20.3)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (6.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pip in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Azure ML Workspace Configuration\n",
        "- Authenticate and initialize Azure Machine Learning client with either `DefaultAzureCredential` or `InteractiveBrowserCredential` for secure access to Azure services.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1709624461465
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directory Setup for Scripts\n",
        "- Create directories for organizing script files and output data, promoting effective project management and data handling."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = '../src'\n",
        "output_folder = '../output'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "../src folder created\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1709624463372
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Prediction Pipeline Component Script\n",
        "- Develop a Python script to predict sentiment from textual dialogue using a pre-trained language model. This script covers model and tokenizer initialization, prompt construction, sentiment prediction, and output handling.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/sentiment_prediction.py\n",
        "import mlflow\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def predict_sentiment(dialogue, output_path, model_name='google/flan-t5-base'):\n",
        "    # Initialize tokeniazer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Constructing a 5-shot prompt with examples\n",
        "    start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "    Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "    Sentiment: Neutral\n",
        "\n",
        "    Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: '''\n",
        "    \n",
        "    end_prompt = '\\nSentiment: '\n",
        "    \n",
        "    # Construct the full prompt with the user-provided dialogue\n",
        "    prompt = start_prompt + '\"' + dialogue + '\"' + end_prompt \n",
        "\n",
        "    # Tokenize input dialogue\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate prediction\n",
        "    output = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "    \n",
        "    # Decode and print the prediction\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Write the predicted sentiment to the specified output file\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write('Text: ' + dialogue + '\\nPredicted Sentiment: ' + decoded_output + '\\n')\n",
        "\n",
        "def main():\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description=\"Predict sentiment from input dialogue\")\n",
        "    parser.add_argument(\"--dialogue\", type=str, required=True, help=\"Input dialogue for sentiment prediction\")\n",
        "    parser.add_argument(\"--output\", type=str, required=True, help=\"Output file path for sentiment prediction\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Predict sentiment and write to output\n",
        "    predict_sentiment(args.dialogue, args.output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/sentiment_prediction.py\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test script\n",
        "!python $script_folder/sentiment_prediction.py --dialogue \"I love this book!\" --output $output_folder\"/output.txt\"\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024/03/04 10:30:43 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n2024-03-04 10:30:43.872746: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-04 10:30:46.159017: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-03-04 10:30:46.811375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-03-04 10:30:46.811420: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-03-04 10:30:52.220737: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-03-04 10:30:52.220949: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-03-04 10:30:52.220970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2024/03/04 10:30:56 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n2024/03/04 10:30:57 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n2024/03/04 10:30:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2024/03/04 10:31:01 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n2024/03/04 10:31:14 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n2024/03/04 10:31:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709548277051
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../src/conda-env.yml\n",
        "name: basic-env-cpu\n",
        "channels:\n",
        "  - conda-forge\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - pip\n",
        "  - pip:\n",
        "    - torch==1.13.1\n",
        "    - rouge_score==0.1.2\n",
        "  - python=3.10.11\n",
        "  - datasets==2.17.0\n",
        "  - torchdata==0.5.1\n",
        "  - transformers==4.27.2\n",
        "  - evaluate==0.4.0\n",
        "  - loralib==0.1.1\n",
        "  - peft==0.3.0\n",
        "  - mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create new environment using base Docker image and conda specs.\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:latest\",\n",
        "    conda_file=\"../src/conda-env.yml\",\n",
        "    name=\"docker-image-llm\",\n",
        "    description=\"llm from docker\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import ComputeInstance\n",
        "\n",
        "\n",
        "ci = ComputeInstance(\n",
        "    name=\"compute-instance\", \n",
        "    size=\"Standard_E4ds_v4\"\n",
        ")\n",
        "ml_client.begin_create_or_update(ci).result()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../sentiment_prediction.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: sentiment_prediction_merged\n",
        "display_name: Sentiment Prediction with Integrated Tokenization\n",
        "version: 3\n",
        "type: command\n",
        "inputs:\n",
        "  dialogue: \n",
        "    type: string\n",
        "outputs:\n",
        "  sentiment_output:\n",
        "    type: uri_file\n",
        "code: ./src\n",
        "environment: azureml:docker-image-llm@latest\n",
        "compute: azureml:cpu-cluster\n",
        "command: >-\n",
        "  python sentiment_prediction.py \n",
        "  --dialogue ${{inputs.dialogue}}\n",
        "  --output ${{outputs.sentiment_output}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../sentiment_prediction.yml\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def sentiment_prediction(pipeline_job_input):\n",
        "    sentiment = predict_sentiment_segment(dialogue=pipeline_job_input)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_predict_sentiment_data\": sentiment.outputs.sentiment_output,\n",
        "        \n",
        "    }\n",
        "\n",
        "# Example usage with a direct string input for the dialogue\n",
        "pipeline_job = sentiment_prediction(pipeline_job_input='\"Movie is rathet bad!\"')\n",
        "pipeline_job.settings.default_compute = \"compute-instance\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "predict_sentiment_segment = load_component(source=parent_dir + \"../sentiment_prediction.yml\")\n",
        "\n",
        "# register component\n",
        "prep = ml_client.components.create_or_update(predict_sentiment_segment, version='6')"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1709015832518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def sentiment_prediction(pipeline_job_input):\n",
        "    sentiment = predict_sentiment_segment(dialogue=pipeline_job_input)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_predict_sentiment_data\": sentiment.outputs.sentiment_output,\n",
        "        \n",
        "    }\n",
        "\n",
        "# Example usage with a direct string input for the dialogue\n",
        "pipeline_job = sentiment_prediction(pipeline_job_input='\"Movie is rathet bad!\"')\n",
        "pipeline_job.settings.default_compute = \"compute-instance\""
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709015835758
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"sentiment_prediction\"\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f300ffbf5b0>}, 'outputs': {'pipeline_job_predict_sentiment_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f300ffbf580>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f300ffbf4f0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'sentiment_prediction', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_predict_sentiment_data': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'sentiment': Command({'parameters': {}, 'init': False, 'name': 'sentiment', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f300ff25240>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'dialogue': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'sentiment_output': '${{parent.outputs.pipeline_job_predict_sentiment_data}}'}, 'inputs': {'dialogue': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f300ffbf760>}, 'outputs': {'sentiment_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f301c116260>}, 'component': 'azureml_anonymous:2d227dc3-faa8-4948-bcf6-19c46f4b92ab', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '115284f9-9688-4eec-88f1-bd80390b9a93', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'busy_chicken_qw3fty2g06', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/falko13/Gen.Ai-APP-in-Azure.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'c8134dbc4eadd7865a07d95bc43e9914f7c8c1f8', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{\"pipeline_job_input\":\"\\\\\"Movie is rathet bad!\\\\\"\"}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'compute-instance', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/jobs/busy_chicken_qw3fty2g06', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f300ffbf550>, 'serialize': <msrest.serialization.Serializer object at 0x7f300ffbf220>, 'display_name': 'sentiment_prediction', 'experiment_name': 'sentiment_prediction', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://australiasoutheast.api.azureml.ms/mlflow/v1.0/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/busy_chicken_qw3fty2g06?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&tid=df64ec22-8387-4f67-874d-a8321645e4ba', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>sentiment_prediction</td><td>busy_chicken_qw3fty2g06</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/busy_chicken_qw3fty2g06?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&amp;tid=df64ec22-8387-4f67-874d-a8321645e4ba\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709015843005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #CLI2 version of creating component and pipeline\n",
        "# !az extension add --name ml -y\n",
        "# output = %sx az ml component list \\\n",
        "#         --resource-group \"cloud-shell-storage-southeastasia\" \\\n",
        "#         --workspace-name \"oksana_ml\"\n",
        "# print(output)\n",
        "# !az ml component create --file ../sentiment_prediction.yml\n",
        "# !az ml job create --file ../pipeline_sentiment_prediction.yml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to register the 'google/flan-t5-base' model for sentiment analysis with MLflow and Azure ML for future online inference\n",
        "\n",
        "# Importing necessary libraries for Azure ML, MLflow, and model loading\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import mlflow\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "def register_model_with_mlflow(model_name='google/flan-t5-base', model_save_path='saved_models/flan_t5_base'):\n",
        "    \"\"\"\n",
        "    Load a pre-trained model and tokenizer from Hugging Face, save them locally,\n",
        "    and register the model with MLflow for tracking and version control.\n",
        "    \n",
        "    Parameters:\n",
        "    - model_name (str): Identifier for the pre-trained model on Hugging Face.\n",
        "    - model_save_path (str): Local directory path for saving the model and tokenizer.\n",
        "    \n",
        "    Returns:\n",
        "    - mlflow_model_name (str): The name of the model registered in MLflow, adjusted for compatibility.\n",
        "    \"\"\"\n",
        "    # Adjusting model_name for MLflow compatibility (replacing '/' with '_')\n",
        "    mlflow_model_name = model_name.replace('/', '_')\n",
        "\n",
        "    # Loading the tokenizer and model from Hugging Face Transformers\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    \n",
        "    # Saving the tokenizer and model locally\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    model.save_pretrained(model_save_path)\n",
        "    \n",
        "    # Logging the model to MLflow with the adjusted name\n",
        "    mlflow.pytorch.log_model(pytorch_model=model, artifact_path=\"models\", registered_model_name=mlflow_model_name)\n",
        "    \n",
        "    return mlflow_model_name\n",
        "\n",
        "def main():\n",
        "    # Setting up MLflow: Configuring the tracking URI and experiment name\n",
        "    mlflow.set_tracking_uri(\"your_mlflow_tracking_uri\")  # Replace with actual MLflow tracking URI\n",
        "    mlflow.set_experiment(\"ModelRegistrationExperiment\")\n",
        "\n",
        "    # Starting an MLflow run to log model information\n",
        "    with mlflow.start_run():\n",
        "        mlflow_model_name = register_model_with_mlflow()\n",
        "\n",
        "    # Fetching the latest run from the experiment for artifact URI\n",
        "    experiment = mlflow.get_experiment_by_name(\"ModelRegistrationExperiment\")\n",
        "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "    latest_run_id = runs.iloc[0]['run_id']  # Assuming the latest run contains the relevant model\n",
        "\n",
        "    # Extracting the model artifact URI from the latest run information\n",
        "    run_info = mlflow.get_run(latest_run_id)\n",
        "    model_artifact_uri = run_info.info.artifact_uri + \"/models\"\n",
        "\n",
        "\n",
        "    # Defining Azure ML model metadata for registration\n",
        "    azure_model = Model(\n",
        "        name=mlflow_model_name,  # Name of the model in Azure ML registry\n",
        "        description=\"Pre-trained sentiment analysis model registered via MLflow.\",\n",
        "        # type=\"mlflow_model\",  # Model type indicating it's tracked by MLflow\n",
        "        type=AssetTypes.MLFLOW_MODEL,\n",
        "        path=model_artifact_uri,  # Path to the model artifact for registration\n",
        "        version=\"4\"  # Model version\n",
        "    )\n",
        "    \n",
        "    # Registering the model in Azure ML\n",
        "    registered_model = ml_client.models.create_or_update(azure_model)\n",
        "    print(f\"Model registered successfully in Azure ML: {registered_model.name}, Version: {registered_model.version}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nRegistered model 'google_flan-t5-base' already exists. Creating a new version of this model...\n2024/02/28 08:46:07 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: google_flan-t5-base, version 12\nCreated version '12' of model 'google_flan-t5-base'.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks/your_mlflow_tracking_uri/425685742346182231/c4cee08203d24dbb8228e09e5b9eba9f/artifacts/models' 'https://oksanaml7538443157.blob.core.windows.net/azureml-blobstore-ce49baec-5e26-4e4d-9649-a752220012af/LocalUpload/010073a314b62c74cc0f51f69afdb924/models' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n\u001b[32mUploading models (990.48 MBs): 100%|██████████| 990481691/990481691 [00:14<00:00, 70252039.26it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model registered successfully in Azure ML: google_flan-t5-base, Version: 4\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709110003197
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define and create an endpoint\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
        "import datetime\n",
        "\n",
        "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"Online endpoint for sentiment extraction\",\n",
        "    auth_mode=\"key\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709189873276
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:b0f9cdd3-a918-48d8-898d-ffe73bdb7af2?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fcc6cb23520>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fcc6cb23a90>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Bad pipe message: %s [b'\\xe2\\xefR\\x99Ix\\xbe\\x92\\xde\\x1b\\xbe\\xac}\\xa1L\\xb9\\xe9D \\x89t']\nBad pipe message: %s [b\"_\\xc6\\xc9\\x00XA\\x1b#\\x92~\\xddqi\\xffHUj\\x07\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\"]\nBad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'\\xcc\\xc4;\\xca\\xc9$+$\\xe6\\xcb\\x18\\xc5V\\r#\\xb6M7\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00']\nBad pipe message: %s [b'\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00']\nBad pipe message: %s [b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\nBad pipe message: %s [b'\\xe0W\\xac\\xba\\xce\\x88\\xc9\\xce\\xf2\\xee\\xcc\\xc2HgG\\x151', b'\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00']\nBad pipe message: %s [b'\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15']\nBad pipe message: %s [b'\\xfe\\xa1\\xb6)9=\\x18\\xf0\\\\s\\xc5\\x97\\xa5\\xc0\\x8f\"\\x00\\xe0\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00']\nBad pipe message: %s [b'\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\nBad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\nBad pipe message: %s [b'\\xa4\\xba\\xac\\x9b\\xb9\\xd0/\\x01T:\\x81?\\xe4\\x8b\\xb0\\x87(\\xb9\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0']\nBad pipe message: %s [b'\\x05']\nBad pipe message: %s [b\"\\xd4p\\xbf\\x038_/\\xaaN%\\x9c|M1;\\xc4\\x0c\\xbc\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\", b'']\nBad pipe message: %s [b\"eJH&\\xd8^2\\xb4\\x08M\\xdc\\xfd\\x05\\x19\\xe8\\xbd\\xf0\\x07\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00\", b'\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17']\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709189981284
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ml_client.online_endpoints.get(name = \"endpoint-02290657909631\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:b0f9cdd3-a918-48d8-898d-ffe73bdb7af2?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f08a5beba60>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f08a5a1f190>, 'traffic': {'blue': 0}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709518569545
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configure the deployment\n",
        "from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "model_name = \"google_flan-t5-base\"  \n",
        "models = ml_client.models.list(name=model_name)\n",
        "latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=\"endpoint-02290657909631\",\n",
        "    model=latest_model,\n",
        "    instance_type=\"Standard_D2as_v4\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "#Create deployment\n",
        "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint endpoint-02290657909631 exists\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "........................................................................................................"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "ManagedOnlineDeployment({'private_network_connection': None, 'provisioning_state': 'Succeeded', 'endpoint_name': 'endpoint-02290657909631', 'type': 'Managed', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/od:ce49baec-5e26-4e4d-9649-a752220012af:f4a1c2ad-dc5d-49f6-9516-67c68151b1b4?api-version=2023-04-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631/deployments/blue', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f4524fea590>, 'model': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/models/google_flan-t5-base/versions/4', 'code_configuration': None, 'environment': None, 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': <azure.ai.ml.entities._deployment.scale_settings.DefaultScaleSettings object at 0x7f4524feaad0>, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7f4524fea920>, 'liveness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f4524fe8790>, 'readiness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f4524feaaa0>, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'Standard_D2as_v4', 'data_collector': None, 'egress_public_network_access': 'Enabled'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709286482459
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# blue deployment takes 100 traffic\n",
        "endpoint = ml_client.online_endpoints.get(name = \"endpoint-02290657909631\")\n",
        "endpoint.traffic = {\"blue\": 0}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\nReadonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:e11d5fac-bda9-48ed-b46e-cd691e507940?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fede8f8e230>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fede8dc9090>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709552026387
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/sentiment_analysis.py\n",
        "import mlflow.pyfunc\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class SentimentAnalysisModel(mlflow.pyfunc.PythonModel):\n",
        "    \n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    def predict(self, context, model_input):\n",
        "        dialogue = model_input.iloc[0]['text']  # Assuming input is a DataFrame with a 'text' column\n",
        "        prompt = self.construct_prompt(dialogue)\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "        output = self.model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "        decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return decoded_output\n",
        "    \n",
        "    def construct_prompt(self, dialogue):\n",
        "        start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "        Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "        Sentiment: Neutral\n",
        "\n",
        "        Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Comment: '''\n",
        "        \n",
        "        end_prompt = '\\nSentiment: '\n",
        "        return start_prompt + '\"' + dialogue + '\"' + end_prompt\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ../src/sentiment_analysis.py\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709541835995
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../src/conda-env-mini.yml\n",
        "channels:\n",
        "- conda-forge\n",
        "dependencies:\n",
        "- python=3.10.11\n",
        "- pip<=24.0\n",
        "- pip:\n",
        "  - mlflow==2.4\n",
        "  - cloudpickle==2.2.1\n",
        "  - torch==1.13.1\n",
        "  - transformers==4.27.2\n",
        "name: mlflow-env\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/conda-env-mini.yml\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from mlflow.models.signature import infer_signature\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "sentiment_model = SentimentAnalysisModel(model_name=model_name)\n",
        "\n",
        "# input and output\n",
        "example_input = pd.DataFrame({\"text\": [\"This is a great movie!\"]})\n",
        "example_output = pd.DataFrame({\"sentiment\": [\"Positive\"]})\n",
        "\n",
        "# Infer signature using example input and output\n",
        "signature = infer_signature(example_input, example_output)\n",
        "\n",
        "\n",
        "# Log the model\n",
        "with mlflow.start_run():\n",
        "    mlflow.pyfunc.log_model(\n",
        "        artifact_path=\"sentiment_analysis_model\",\n",
        "        python_model=sentiment_model,\n",
        "        code_path=[\"../src/sentiment_analysis.py\"],  \n",
        "        conda_env=\"../src/conda-env-mini.yml\",  \n",
        "        signature=signature  ,\n",
        "        registered_model_name = \"google_flan-t5-base\" #Register model at the same time\n",
        "\n",
        "    )\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Registered model 'google_flan-t5-base' already exists. Creating a new version of this model...\n2024/03/04 13:08:16 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: google_flan-t5-base, version 8\nCreated version '8' of model 'google_flan-t5-base'.\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709557696686
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/score.py\n",
        "import json\n",
        "import pandas as pd\n",
        "import mlflow.pyfunc\n",
        "\n",
        "# Path where the MLflow model is saved, adjust as necessary\n",
        "model_path = \"models:/SentimentAnalysisModel/Production\"\n",
        "\n",
        "# Global variable for holding the model\n",
        "model = None\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # Load the MLflow model into the global variable\n",
        "    model = mlflow.pyfunc.load_model(model_path)\n",
        "\n",
        "def run(raw_data):\n",
        "    try:\n",
        "        # Parse the incoming JSON data\n",
        "        input_data = json.loads(raw_data)\n",
        "        \n",
        "        # Extract the 'text' value to conform with the input signature\n",
        "        # Creating a DataFrame to match the expected input format for the predict function\n",
        "        data = pd.DataFrame({\"text\": [input_data['text']]})\n",
        "        \n",
        "        # Use the global model to predict the sentiment\n",
        "        prediction = model.predict(data)\n",
        "        \n",
        "        # Format the output to match the output signature\n",
        "        # Assuming prediction is returned as a string\n",
        "        result = {\"sentiment\": prediction[0]}\n",
        "        \n",
        "        # Return the prediction as JSON\n",
        "        return json.dumps(result)\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        return json.dumps({\"error\": error})\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ../src/score.py\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configure the deployment\n",
        "from azure.ai.ml.entities import Model, ManagedOnlineDeployment, CodeConfiguration\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "model_name = \"google_flan-t5-base\"  \n",
        "models = ml_client.models.list(name=model_name)\n",
        "latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "# Assuming 'score.py' and any other necessary files are located in a directory named 'deployment_files'\n",
        "code_configuration = CodeConfiguration(\n",
        "    code=\"Users/opanasenko2084/Gen.Ai-APP-in-Azure/src\",  # Path to the directory containing your 'score.py' and any other necessary files\n",
        "    scoring_script=\"score.py\"  # Name of the scoring script\n",
        ")\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",  # avoiding green/blue approach and deploying directly to blue production due quota limitation\n",
        "    endpoint_name=\"endpoint-02290657909631\",\n",
        "    model=latest_model,\n",
        "    instance_type=\"Standard_D2as_v4\",\n",
        "    instance_count=1,\n",
        "    code_configuration=code_configuration  # Add the code configuration to the deployment\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "..................................................................."
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709606662469
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Configure the deployment\n",
        "# from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
        "# from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# model_name = \"google_flan-t5-base\"  \n",
        "# models = ml_client.models.list(name=model_name)\n",
        "# latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "# blue_deployment = ManagedOnlineDeployment(\n",
        "#     name=\"blue\", #avoiding green/blue approach and deploying directly to blue production due quota limitation\n",
        "#     endpoint_name=\"endpoint-02290657909631\",\n",
        "#     model=latest_model,\n",
        "#     instance_type=\"Standard_D2as_v4\",\n",
        "#     instance_count=1\n",
        "# )\n",
        "\n",
        "# #Create deployment\n",
        "# ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709606624705
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# blue deployment takes 100 traffic\n",
        "endpoint_name = \"endpoint-02290657909631\"\n",
        "endpoint= ml_client.online_endpoints.get(name=endpoint_name)\n",
        "\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\nReadonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:89a5df1d-4c37-458c-a4f8-6942fcf38a97?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f02a427d540>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f02a427cc70>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709624962166
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Define the endpoint name (change this to your endpoint's name)\n",
        "endpoint_name = \"endpoint-02290657909631\"\n",
        "deployment_name = \"blue\"\n",
        "# Construct the scoring URI\n",
        "scoring_uri = ml_client.online_endpoints.get(name=endpoint_name).scoring_uri\n",
        "\n",
        "\n",
        "# Define the input data as a Python dictionary\n",
        "input_data = {\n",
        "  \"input_data\": {\n",
        "    \"columns\": [\"text\"],\n",
        "    \"index\": [0],\n",
        "    \"data\": [\n",
        "      [\"This is the worst customer service experience I've ever had.\"]\n",
        "    ]\n",
        "  },\n",
        "  \"params\": {}\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a JSON-formatted string\n",
        "input_json = json.dumps(input_data)\n",
        "\n",
        "# Create a temporary file to hold the JSON data\n",
        "with tempfile.NamedTemporaryFile(mode='w', delete=True) as temp_file:\n",
        "    temp_file.write(input_json)\n",
        "    temp_file.flush()  # Ensure all data is written to the file\n",
        "\n",
        "    # Use the temporary file's path to send the scoring request\n",
        "    response = ml_client.online_endpoints.invoke(\n",
        "        endpoint_name=endpoint_name,\n",
        "        deployment_name=deployment_name,\n",
        "        request_file=temp_file.name  # Use the path to the temporary file\n",
        "    )\n",
        "\n",
        "    # The temporary file is automatically deleted here, as we exit the 'with' block\n",
        "\n",
        "# Print the response from the scoring request\n",
        "print(\"Response:\", response)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Response: \"Negative\"\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709626556513
        }
      }
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "name": "ml.geospatial.interactive",
        "_defaultOrder": 20,
        "hideHardwareSpecs": true,
        "vcpuNum": 0,
        "_isFastLaunch": false,
        "gpuNum": 0,
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "category": "General purpose",
        "memoryGiB": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}