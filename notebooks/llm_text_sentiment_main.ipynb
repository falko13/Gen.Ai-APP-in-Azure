{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Part 1: Script for Component Creation and Pipeline Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Setting up the Environment\n",
        "- Install necessary libraries including `datasets` for data handling, `torch` and `torchdata` for neural network operations, and `transformers`, `evaluate`, `rouge_score`, `loralib`, and `peft` for model training, evaluation, and enhancement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets==2.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (1.25.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (12.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.20.3)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (6.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pip in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet\n",
        "\n",
        "    \n",
        "# Installing the Reinforcement Learning library directly from github.\n",
        "# %pip install git+https://github.com/lvwerra/trl.git@25fa1bd  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Azure ML Workspace Configuration\n",
        "- Authenticate and initialize Azure Machine Learning client with either `DefaultAzureCredential` or `InteractiveBrowserCredential` for secure access to Azure services.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1709712508883
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        }
      ],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Directory Setup for Scripts\n",
        "- Create directories for organizing script files and output data, promoting effective project management and data handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1709705334081
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../src folder created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = '../src'\n",
        "output_folder = '../output'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Sentiment Prediction Pipeline Component Script\n",
        "- Develop a Python script to predict sentiment from textual dialogue using a pre-trained language model. This script covers model and tokenizer initialization, prompt construction, sentiment prediction, and output handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../src/sentiment_prediction.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/sentiment_prediction.py\n",
        "import mlflow\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def predict_sentiment(dialogue, output_path, model_name='google/flan-t5-base'):\n",
        "    # Initialize tokeniazer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Constructing a 5-shot prompt with examples\n",
        "    start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "    Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "    Sentiment: Neutral\n",
        "\n",
        "    Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: '''\n",
        "    \n",
        "    end_prompt = '\\nSentiment: '\n",
        "    \n",
        "    # Construct the full prompt with the user-provided dialogue\n",
        "    prompt = start_prompt + '\"' + dialogue + '\"' + end_prompt \n",
        "\n",
        "    # Tokenize input dialogue\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate prediction\n",
        "    output = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "    \n",
        "    # Decode and print the prediction\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Write the predicted sentiment to the specified output file\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write('Text: ' + dialogue + '\\nPredicted Sentiment: ' + decoded_output + '\\n')\n",
        "\n",
        "def main():\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description=\"Predict sentiment from input dialogue\")\n",
        "    parser.add_argument(\"--dialogue\", type=str, required=True, help=\"Input dialogue for sentiment prediction\")\n",
        "    parser.add_argument(\"--output\", type=str, required=True, help=\"Output file path for sentiment prediction\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Predict sentiment and write to output\n",
        "    predict_sentiment(args.dialogue, args.output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the Sentiment Prediction Script\n",
        "- Test the functionality of the sentiment prediction script with a sample dialogue, ensuring accurate sentiment analysis and output generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1709548277051
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024/03/04 10:30:43 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n",
            "2024-03-04 10:30:43.872746: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-04 10:30:46.159017: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-03-04 10:30:46.811375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-03-04 10:30:46.811420: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-03-04 10:30:52.220737: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2024-03-04 10:30:52.220949: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2024-03-04 10:30:52.220970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2024/03/04 10:30:56 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
            "2024/03/04 10:30:57 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
            "2024/03/04 10:30:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "2024/03/04 10:31:01 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
            "2024/03/04 10:31:14 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n",
            "2024/03/04 10:31:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n"
          ]
        }
      ],
      "source": [
        "#Test script\n",
        "!python $script_folder/sentiment_prediction.py --dialogue \"I love this book!\" --output $output_folder\"/output.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Prepare Resource Scripts: Environment, Compute, and Component Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Environment Configuration File**  \n",
        "- Defines necessary dependencies for the sentiment analysis model, ensuring consistent environments for training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile ../src/conda-env.yml\n",
        "name: basic-env-cpu\n",
        "channels:\n",
        "  - conda-forge\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - pip\n",
        "  - pip:\n",
        "    - torch==1.13.1\n",
        "    - rouge_score==0.1.2\n",
        "  - python=3.10.11\n",
        "  - datasets==2.17.0\n",
        "  - torchdata==0.5.1\n",
        "  - transformers==4.27.2\n",
        "  - evaluate==0.4.0\n",
        "  - loralib==0.1.1\n",
        "  - peft==0.3.0\n",
        "  - mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Azure ML Environment Setup**  \n",
        "- Utilizes the Azure ML SDK to create an environment with specified conda and Docker image settings, facilitating model operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# create new environment using base Docker image and conda specs.\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:latest\",\n",
        "    conda_file=\"../src/conda-env.yml\",\n",
        "    name=\"docker-image-llm\",\n",
        "    description=\"llm from docker\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Compute Instance Configuration**  \n",
        "- Configures a compute instance in Azure ML, allocating necessary computational resources for model tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import ComputeInstance\n",
        "\n",
        "\n",
        "ci = ComputeInstance(\n",
        "    name=\"compute-instance\", \n",
        "    size=\"Standard_E4ds_v4\"\n",
        ")\n",
        "ml_client.begin_create_or_update(ci).result()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Sentiment Prediction Component Configuration**  \n",
        "- Creates a YAML configuration for the sentiment prediction component, detailing its execution within Azure ML pipelines, including inputs, outputs, and runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../sentiment_prediction.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../sentiment_prediction.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: sentiment_prediction_merged\n",
        "display_name: Sentiment Prediction with Integrated Tokenization\n",
        "version: 3\n",
        "type: command\n",
        "inputs:\n",
        "  dialogue: \n",
        "    type: string\n",
        "outputs:\n",
        "  sentiment_output:\n",
        "    type: uri_file\n",
        "code: ./src\n",
        "environment: azureml:docker-image-llm@latest\n",
        "compute: azureml:cpu-cluster\n",
        "command: >-\n",
        "  python sentiment_prediction.py \n",
        "  --dialogue ${{inputs.dialogue}}\n",
        "  --output ${{outputs.sentiment_output}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Register Component, Configure Pipeline, and Start Command Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# from azure.ai.ml import Input\n",
        "# from azure.ai.ml.constants import AssetTypes\n",
        "# from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "# @pipeline()\n",
        "# def sentiment_prediction(pipeline_job_input):\n",
        "#     sentiment = predict_sentiment_segment(dialogue=pipeline_job_input)\n",
        "\n",
        "#     return {\n",
        "#         \"pipeline_job_predict_sentiment_data\": sentiment.outputs.sentiment_output,\n",
        "        \n",
        "#     }\n",
        "\n",
        "# # Example usage with a direct string input for the dialogue\n",
        "# pipeline_job = sentiment_prediction(pipeline_job_input='\"Movie is rathet bad!\"')\n",
        "# pipeline_job.settings.default_compute = \"compute-instance\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Component Registration**  \n",
        "- Utilizes `load_component` to load the sentiment prediction component from YAML and registers it with Azure ML, ensuring the component is versioned and reusable across ML workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1709015832518
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "predict_sentiment_segment = load_component(source=parent_dir + \"../sentiment_prediction.yml\")\n",
        "\n",
        "# register component\n",
        "prep = ml_client.components.create_or_update(predict_sentiment_segment, version='6')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pipeline Creation**  \n",
        "- Defines an ML pipeline using the Azure ML SDK, incorporating the registered sentiment prediction component. This pipeline takes textual input and processes it through the sentiment analysis model, demonstrating how to orchestrate ML tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1709015835758
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def sentiment_prediction(pipeline_job_input):\n",
        "    sentiment = predict_sentiment_segment(dialogue=pipeline_job_input)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_predict_sentiment_data\": sentiment.outputs.sentiment_output,\n",
        "        \n",
        "    }\n",
        "\n",
        "# Example usage with a direct string input for the dialogue\n",
        "pipeline_job = sentiment_prediction(pipeline_job_input='\"Movie is rathet bad!\"')\n",
        "pipeline_job.settings.default_compute = \"compute-instance\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pipeline Execution**  \n",
        "- Submits the configured pipeline as a job to the Azure ML workspace. This step initiates the execution of the sentiment prediction workflow, showcasing how to operationalize ML components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1709015843005
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>sentiment_prediction</td><td>busy_chicken_qw3fty2g06</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/busy_chicken_qw3fty2g06?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&amp;tid=df64ec22-8387-4f67-874d-a8321645e4ba\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f300ffbf5b0>}, 'outputs': {'pipeline_job_predict_sentiment_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f300ffbf580>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f300ffbf4f0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'sentiment_prediction', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_predict_sentiment_data': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'sentiment': Command({'parameters': {}, 'init': False, 'name': 'sentiment', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f300ff25240>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'dialogue': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'sentiment_output': '${{parent.outputs.pipeline_job_predict_sentiment_data}}'}, 'inputs': {'dialogue': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f300ffbf760>}, 'outputs': {'sentiment_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f301c116260>}, 'component': 'azureml_anonymous:2d227dc3-faa8-4948-bcf6-19c46f4b92ab', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '115284f9-9688-4eec-88f1-bd80390b9a93', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'busy_chicken_qw3fty2g06', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/falko13/Gen.Ai-APP-in-Azure.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'c8134dbc4eadd7865a07d95bc43e9914f7c8c1f8', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{\"pipeline_job_input\":\"\\\\\"Movie is rathet bad!\\\\\"\"}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'compute-instance', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/jobs/busy_chicken_qw3fty2g06', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f300ffbf550>, 'serialize': <msrest.serialization.Serializer object at 0x7f300ffbf220>, 'display_name': 'sentiment_prediction', 'experiment_name': 'sentiment_prediction', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://australiasoutheast.api.azureml.ms/mlflow/v1.0/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/busy_chicken_qw3fty2g06?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&tid=df64ec22-8387-4f67-874d-a8321645e4ba', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"sentiment_prediction\"\n",
        ")\n",
        "pipeline_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Alternative Pipeline Creation with CLI**  \n",
        "- Outlines an alternative approach for creating and managing components and pipelines using the Azure CLI. This method provides a script-based option for automating the deployment and execution of ML workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# #CLI2 version of creating component and pipeline\n",
        "# !az extension add --name ml -y\n",
        "# output = %sx az ml component list \\\n",
        "#         --resource-group \"cloud-shell-storage-southeastasia\" \\\n",
        "#         --workspace-name \"oksana_ml\"\n",
        "# print(output)\n",
        "# !az ml component create --file ../sentiment_prediction.yml\n",
        "# !az ml job create --file ../pipeline_sentiment_prediction.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Implement Online Inference Endpoint and Deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Registration with MLflow\n",
        "- Registers the 'google/flan-t5-base' model for sentiment analysis within MLflow and Azure ML, tailored for future online inference. This process involves saving a pre-trained model and tokenizer, and registering the model under a pyfunc flavour due to specific preprocessing requirements. (testing registration, actual endpoing model to be registered further)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1709110003197
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "Registered model 'google_flan-t5-base' already exists. Creating a new version of this model...\n",
            "2024/02/28 08:46:07 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: google_flan-t5-base, version 12\n",
            "Created version '12' of model 'google_flan-t5-base'.\n",
            "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
            "\n",
            "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks/your_mlflow_tracking_uri/425685742346182231/c4cee08203d24dbb8228e09e5b9eba9f/artifacts/models' 'https://oksanaml7538443157.blob.core.windows.net/azureml-blobstore-ce49baec-5e26-4e4d-9649-a752220012af/LocalUpload/010073a314b62c74cc0f51f69afdb924/models' \n",
            "\n",
            "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
            "\u001b[32mUploading models (990.48 MBs): 100%|██████████| 990481691/990481691 [00:14<00:00, 70252039.26it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model registered successfully in Azure ML: google_flan-t5-base, Version: 4\n"
          ]
        }
      ],
      "source": [
        "# Script to register the 'google/flan-t5-base' model for sentiment analysis with MLflow and Azure ML for future online inference\n",
        "\n",
        "# Importing necessary libraries for Azure ML, MLflow, and model loading\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import mlflow\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "def register_model_with_mlflow(model_name='google/flan-t5-base', model_save_path='saved_models/flan_t5_base'):\n",
        "    \"\"\"\n",
        "    Load a pre-trained model and tokenizer from Hugging Face, save them locally,\n",
        "    and register the model with MLflow for tracking and version control.\n",
        "    \n",
        "    Parameters:\n",
        "    - model_name (str): Identifier for the pre-trained model on Hugging Face.\n",
        "    - model_save_path (str): Local directory path for saving the model and tokenizer.\n",
        "    \n",
        "    Returns:\n",
        "    - mlflow_model_name (str): The name of the model registered in MLflow, adjusted for compatibility.\n",
        "    \"\"\"\n",
        "    # Adjusting model_name for MLflow compatibility (replacing '/' with '_')\n",
        "    mlflow_model_name = model_name.replace('/', '_')\n",
        "\n",
        "    # Loading the tokenizer and model from Hugging Face Transformers\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    \n",
        "    # Saving the tokenizer and model locally\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    model.save_pretrained(model_save_path)\n",
        "    \n",
        "    # Logging the model to MLflow with the adjusted name\n",
        "    mlflow.pytorch.log_model(pytorch_model=model, artifact_path=\"models\", registered_model_name=mlflow_model_name)\n",
        "    \n",
        "    return mlflow_model_name\n",
        "\n",
        "def main():\n",
        "    # Setting up MLflow: Configuring the tracking URI and experiment name\n",
        "    mlflow.set_tracking_uri(\"your_mlflow_tracking_uri\")  # Replace with actual MLflow tracking URI\n",
        "    mlflow.set_experiment(\"ModelRegistrationExperiment\")\n",
        "\n",
        "    # Starting an MLflow run to log model information\n",
        "    with mlflow.start_run():\n",
        "        mlflow_model_name = register_model_with_mlflow()\n",
        "\n",
        "    # Fetching the latest run from the experiment for artifact URI\n",
        "    experiment = mlflow.get_experiment_by_name(\"ModelRegistrationExperiment\")\n",
        "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "    latest_run_id = runs.iloc[0]['run_id']  # Assuming the latest run contains the relevant model\n",
        "\n",
        "    # Extracting the model artifact URI from the latest run information\n",
        "    run_info = mlflow.get_run(latest_run_id)\n",
        "    model_artifact_uri = run_info.info.artifact_uri + \"/models\"\n",
        "\n",
        "\n",
        "    # Defining Azure ML model metadata for registration\n",
        "    azure_model = Model(\n",
        "        name=mlflow_model_name,  # Name of the model in Azure ML registry\n",
        "        description=\"Pre-trained sentiment analysis model registered via MLflow.\",\n",
        "        # type=\"mlflow_model\",  # Model type indicating it's tracked by MLflow\n",
        "        type=AssetTypes.MLFLOW_MODEL,\n",
        "        path=model_artifact_uri,  # Path to the model artifact for registration\n",
        "        version=\"4\"  # Model version\n",
        "    )\n",
        "    \n",
        "    # Registering the model in Azure ML\n",
        "    registered_model = ml_client.models.create_or_update(azure_model)\n",
        "    print(f\"Model registered successfully in Azure ML: {registered_model.name}, Version: {registered_model.version}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Endpoint Creation\n",
        "- Defines and creates a managed online endpoint for sentiment analysis, specifying authentication modes and descriptions to set up an accessible and secure endpoint for model inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1709189873276
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Define and create an endpoint\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
        "import datetime\n",
        "\n",
        "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"Online endpoint for sentiment extraction\",\n",
        "    auth_mode=\"key\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1709189981284
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:b0f9cdd3-a918-48d8-898d-ffe73bdb7af2?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fcc6cb23520>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fcc6cb23a90>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Bad pipe message: %s [b'\\xe2\\xefR\\x99Ix\\xbe\\x92\\xde\\x1b\\xbe\\xac}\\xa1L\\xb9\\xe9D \\x89t']\n",
            "Bad pipe message: %s [b\"_\\xc6\\xc9\\x00XA\\x1b#\\x92~\\xddqi\\xffHUj\\x07\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\"]\n",
            "Bad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
            "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
            "Bad pipe message: %s [b'']\n",
            "Bad pipe message: %s [b'', b'\\x02']\n",
            "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
            "Bad pipe message: %s [b'\\xcc\\xc4;\\xca\\xc9$+$\\xe6\\xcb\\x18\\xc5V\\r#\\xb6M7\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00']\n",
            "Bad pipe message: %s [b'\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00']\n",
            "Bad pipe message: %s [b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
            "Bad pipe message: %s [b'\\xe0W\\xac\\xba\\xce\\x88\\xc9\\xce\\xf2\\xee\\xcc\\xc2HgG\\x151', b'\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00']\n",
            "Bad pipe message: %s [b'\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15']\n",
            "Bad pipe message: %s [b'\\xfe\\xa1\\xb6)9=\\x18\\xf0\\\\s\\xc5\\x97\\xa5\\xc0\\x8f\"\\x00\\xe0\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00']\n",
            "Bad pipe message: %s [b'\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
            "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
            "Bad pipe message: %s [b'\\xa4\\xba\\xac\\x9b\\xb9\\xd0/\\x01T:\\x81?\\xe4\\x8b\\xb0\\x87(\\xb9\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0']\n",
            "Bad pipe message: %s [b'\\x05']\n",
            "Bad pipe message: %s [b\"\\xd4p\\xbf\\x038_/\\xaaN%\\x9c|M1;\\xc4\\x0c\\xbc\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\", b'']\n",
            "Bad pipe message: %s [b\"eJH&\\xd8^2\\xb4\\x08M\\xdc\\xfd\\x05\\x19\\xe8\\xbd\\xf0\\x07\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00\", b'\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17']\n"
          ]
        }
      ],
      "source": [
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1709518569545
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:b0f9cdd3-a918-48d8-898d-ffe73bdb7af2?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f08a5beba60>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f08a5a1f190>, 'traffic': {'blue': 0}, 'mirror_traffic': {}, 'kind': 'Managed'})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "endpoint = ml_client.online_endpoints.get(name = \"endpoint-02290657909631\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1709286482459
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint endpoint-02290657909631 exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "........................................................................................................"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ManagedOnlineDeployment({'private_network_connection': None, 'provisioning_state': 'Succeeded', 'endpoint_name': 'endpoint-02290657909631', 'type': 'Managed', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/od:ce49baec-5e26-4e4d-9649-a752220012af:f4a1c2ad-dc5d-49f6-9516-67c68151b1b4?api-version=2023-04-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631/deployments/blue', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f4524fea590>, 'model': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/models/google_flan-t5-base/versions/4', 'code_configuration': None, 'environment': None, 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': <azure.ai.ml.entities._deployment.scale_settings.DefaultScaleSettings object at 0x7f4524feaad0>, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7f4524fea920>, 'liveness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f4524fe8790>, 'readiness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f4524feaaa0>, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'Standard_D2as_v4', 'data_collector': None, 'egress_public_network_access': 'Enabled'})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# #Configure the deployment\n",
        "# from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
        "# from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# model_name = \"google_flan-t5-base\"  \n",
        "# models = ml_client.models.list(name=model_name)\n",
        "# latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "# blue_deployment = ManagedOnlineDeployment(\n",
        "#     name=\"blue\",\n",
        "#     endpoint_name=\"endpoint-02290657909631\",\n",
        "#     model=latest_model,\n",
        "#     instance_type=\"Standard_D2as_v4\",\n",
        "#     instance_count=1,\n",
        "# )\n",
        "\n",
        "# #Create deployment\n",
        "# ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1709552026387
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
            "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:e11d5fac-bda9-48ed-b46e-cd691e507940?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fede8f8e230>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fede8dc9090>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # blue deployment takes 100 traffic\n",
        "# endpoint = ml_client.online_endpoints.get(name = \"endpoint-02290657909631\")\n",
        "# endpoint.traffic = {\"blue\": 100}\n",
        "# ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Model Preparation for Endpoint\n",
        "- Develops a custom `SentimentAnalysisModel` under the mlflow.pyfunc flavour to accommodate additional preprocessing like custom prompts and tokenization, specifically designed for online inference scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1709541835995
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ../src/sentiment_analysis.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/sentiment_analysis.py\n",
        "import mlflow.pyfunc\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class SentimentAnalysisModel(mlflow.pyfunc.PythonModel):\n",
        "    \n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    def predict(self, context, model_input):\n",
        "        dialogue = model_input.iloc[0]['text']  # Assuming input is a DataFrame with a 'text' column\n",
        "        prompt = self.construct_prompt(dialogue)\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "        output = self.model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "        decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return decoded_output\n",
        "    \n",
        "    def construct_prompt(self, dialogue):\n",
        "        start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "        Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "        Sentiment: Neutral\n",
        "\n",
        "        Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Comment: '''\n",
        "        \n",
        "        end_prompt = '\\nSentiment: '\n",
        "        return start_prompt + '\"' + dialogue + '\"' + end_prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Minimization\n",
        "- Streamlines the conda environment to include only essential dependencies, optimizing resource usage and deployment efficiency for the online inference environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../src/conda-env-mini.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../src/conda-env-mini.yml\n",
        "channels:\n",
        "- conda-forge\n",
        "dependencies:\n",
        "- python=3.10.11\n",
        "- pip<=24.0\n",
        "- pip:\n",
        "  - mlflow==2.4\n",
        "  - cloudpickle==2.2.1\n",
        "  - torch==1.13.1\n",
        "  - transformers==4.27.2\n",
        "name: mlflow-env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLflow Pyfunc Model Registration\n",
        "- Logs the `SentimentAnalysisModel` as an MLflow PyFunc model, incorporating example inputs and outputs to infer model signature. This step finalizes the model's preparation for deployment, including registering it with a simplified environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1709557696686
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Registered model 'google_flan-t5-base' already exists. Creating a new version of this model...\n",
            "2024/03/04 13:08:16 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: google_flan-t5-base, version 8\n",
            "Created version '8' of model 'google_flan-t5-base'.\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "from mlflow.models.signature import infer_signature\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "sentiment_model = SentimentAnalysisModel(model_name=model_name)\n",
        "\n",
        "# input and output\n",
        "example_input = pd.DataFrame({\"text\": [\"This is a great movie!\"]})\n",
        "example_output = pd.DataFrame({\"sentiment\": [\"Positive\"]})\n",
        "\n",
        "# Infer signature using example input and output\n",
        "signature = infer_signature(example_input, example_output)\n",
        "\n",
        "\n",
        "# Log the model\n",
        "with mlflow.start_run():\n",
        "    mlflow.pyfunc.log_model(\n",
        "        artifact_path=\"sentiment_analysis_model\",\n",
        "        python_model=sentiment_model,\n",
        "        code_path=[\"../src/sentiment_analysis.py\"],  \n",
        "        conda_env=\"../src/conda-env-mini.yml\",  \n",
        "        signature=signature  ,\n",
        "        registered_model_name = \"google_flan-t5-base\" #Register model at the same time\n",
        "\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring Script Preparation\n",
        "- Crafts a scoring script (`score.py`) that loads the MLflow model and processes incoming JSON data to perform sentiment analysis, returning structured prediction results. This script acts as the interface between the endpoint and the deployed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ../src/score.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/score.py\n",
        "import json\n",
        "import pandas as pd\n",
        "import mlflow.pyfunc\n",
        "\n",
        "# Path where the MLflow model is saved, adjust as necessary\n",
        "model_path = \"models:/SentimentAnalysisModel/Production\"\n",
        "\n",
        "# Global variable for holding the model\n",
        "model = None\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # Load the MLflow model into the global variable\n",
        "    model = mlflow.pyfunc.load_model(model_path)\n",
        "\n",
        "def run(raw_data):\n",
        "    try:\n",
        "        # Parse the incoming JSON data\n",
        "        input_data = json.loads(raw_data)\n",
        "        \n",
        "        # Extract the 'text' value to conform with the input signature\n",
        "        # Creating a DataFrame to match the expected input format for the predict function\n",
        "        data = pd.DataFrame({\"text\": [input_data['text']]})\n",
        "        \n",
        "        # Use the global model to predict the sentiment\n",
        "        prediction = model.predict(data)\n",
        "        \n",
        "        # Format the output to match the output signature\n",
        "        # Assuming prediction is returned as a string\n",
        "        result = {\"sentiment\": prediction[0]}\n",
        "        \n",
        "        # Return the prediction as JSON\n",
        "        return json.dumps(result)\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        return json.dumps({\"error\": error})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment Configuration\n",
        "- Sets up deployment configurations, including specifying the model, instance type, code configuration, and instance count. This configuration is critical for deploying the model to the online endpoint effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1709700630756
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# #Configure the deployment\n",
        "# from azure.ai.ml.entities import Model, ManagedOnlineDeployment, CodeConfiguration\n",
        "# from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# model_name = \"google_flan-t5-base\"  \n",
        "# models = ml_client.models.list(name=model_name)\n",
        "# latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "\n",
        "# code_configuration = CodeConfiguration(\n",
        "#     code=\"Users/opanasenko2084/Gen.Ai-APP-in-Azure/src\",  # Path to the directory containing your 'score.py' and any other necessary files\n",
        "#     scoring_script=\"score.py\"  # Name of the scoring script\n",
        "# )\n",
        "\n",
        "# blue_deployment = ManagedOnlineDeployment(\n",
        "#     name=\"blue\",  # avoiding green/blue approach and deploying directly to blue production due quota limitation\n",
        "#     endpoint_name=\"endpoint-02290657909631\",\n",
        "#     model=latest_model,\n",
        "#     instance_type=\"Standard_D2as_v4\",\n",
        "#     instance_count=1,\n",
        "#     code_configuration=code_configuration  # Add the code configuration to the deployment\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1709712550972
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Configure the deployment\n",
        "from azure.ai.ml.entities import Model, ManagedOnlineDeployment, CodeConfiguration\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "model_name = \"google_flan-t5-base\"  \n",
        "models = ml_client.models.list(name=model_name)\n",
        "latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "\n",
        "code_configuration = CodeConfiguration(\n",
        "    code=\"Users/opanasenko2084/Gen.Ai-APP-in-Azure/src\",  # Path to the directory containing your 'score.py' and any other necessary files\n",
        "    scoring_script=\"score.py\"  # Name of the scoring script\n",
        ")\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\", #avoiding green/blue approach and deploying directly to blue production due quota limitation\n",
        "    endpoint_name=\"endpoint-02290657909631\",\n",
        "    model=latest_model,\n",
        "    instance_type=\"Standard_D2as_v4\",\n",
        "    instance_count=1,\n",
        "    code_configuration=code_configuration  # Add the code configuration to the deployment\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment and Traffic Management\n",
        "- Executes the deployment to the managed online endpoint and adjusts traffic allocation to ensure the newly deployed model handles incoming inference requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1709712553457
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Create deployment\n",
        "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1709624962166
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
            "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-02290657909631.australiasoutheast.inference.ml.azure.com/swagger.json', 'name': 'endpoint-02290657909631', 'description': 'Online endpoint for sentiment extraction', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/providers/microsoft.machinelearningservices/workspaces/oksana_ml/onlineendpoints/endpoint-02290657909631', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/providers/Microsoft.MachineLearningServices/locations/australiasoutheast/mfeOperationsStatus/oe:ce49baec-5e26-4e4d-9649-a752220012af:89a5df1d-4c37-458c-a4f8-6942fcf38a97?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/onlineEndpoints/endpoint-02290657909631', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f02a427d540>, 'auth_mode': 'key', 'location': 'australiasoutheast', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f02a427cc70>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# blue deployment takes 100 traffic\n",
        "endpoint_name = \"endpoint-02290657909631\"\n",
        "endpoint= ml_client.online_endpoints.get(name=endpoint_name)\n",
        "\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Endpoint Testing\n",
        "- Conducts a test to validate the endpoint's functionality, sending a sample request and verifying the model's ability to return the expected sentiment analysis result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1709626556513
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \"Negative\"\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Define the endpoint name (change this to your endpoint's name)\n",
        "endpoint_name = \"endpoint-02290657909631\"\n",
        "deployment_name = \"blue\"\n",
        "# Construct the scoring URI\n",
        "scoring_uri = ml_client.online_endpoints.get(name=endpoint_name).scoring_uri\n",
        "\n",
        "\n",
        "# Define the input data as a Python dictionary\n",
        "input_data = {\n",
        "  \"input_data\": {\n",
        "    \"columns\": [\"text\"],\n",
        "    \"index\": [0],\n",
        "    \"data\": [\n",
        "      [\"This is the worst customer service experience I've ever had.\"]\n",
        "    ]\n",
        "  },\n",
        "  \"params\": {}\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a JSON-formatted string\n",
        "input_json = json.dumps(input_data)\n",
        "\n",
        "# Create a temporary file to hold the JSON data\n",
        "with tempfile.NamedTemporaryFile(mode='w', delete=True) as temp_file:\n",
        "    temp_file.write(input_json)\n",
        "    temp_file.flush()  # Ensure all data is written to the file\n",
        "\n",
        "    # Use the temporary file's path to send the scoring request\n",
        "    response = ml_client.online_endpoints.invoke(\n",
        "        endpoint_name=endpoint_name,\n",
        "        deployment_name=deployment_name,\n",
        "        request_file=temp_file.name  # Use the path to the temporary file\n",
        "    )\n",
        "\n",
        "    # The temporary file is automatically deleted here, as we exit the 'with' block\n",
        "\n",
        "# Print the response from the scoring request\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1709700102902
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "endpoint.traffic = {\"blue\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Part 6: Batch Version of Endpoint\n",
        "This section focuses on adapting the sentiment analysis model for batch processing and deploying it as a batch endpoint in Azure ML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLClient Initialization\n",
        "- Establishes a connection to Azure ML workspace using Azure credentials, preparing for operations like model registration, environment setup, and deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1709714204106
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        }
      ],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Folder Structure Preparation\n",
        "- Ensures the necessary directory structure is in place for storing scripts and output, facilitating organized code management and execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1709714186984
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../src folder created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = '../src'\n",
        "output_folder = '../output'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Adaptation for Batch Scoring\n",
        "- Adjusts the sentiment analysis model to handle batch input. This version is designed to process multiple inputs in a single batch, returning a DataFrame of sentiment predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ../src/sentiment_analysis_batches.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/sentiment_analysis_batches.py\n",
        "import mlflow.pyfunc\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "class SentimentAnalysisModel(mlflow.pyfunc.PythonModel):\n",
        "    \n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    def predict(self, context, model_input):\n",
        "        # Ensure model_input is a DataFrame\n",
        "        if not isinstance(model_input, pd.DataFrame):\n",
        "            raise ValueError(\"Model input must be a pandas DataFrame\")\n",
        "        \n",
        "        results = []\n",
        "        for index, row in model_input.iterrows():\n",
        "            dialogue = row['text']\n",
        "            prompt = self.construct_prompt(dialogue)\n",
        "            inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "            output = self.model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "            decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            results.append(decoded_output)\n",
        "        \n",
        "        # Return a DataFrame with the results\n",
        "        return pd.DataFrame(results, columns=['sentiment'])\n",
        "    \n",
        "    def construct_prompt(self, dialogue):\n",
        "        start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "        Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "        Sentiment: Neutral\n",
        "\n",
        "        Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Comment: '''\n",
        "        \n",
        "        end_prompt = '\\nSentiment: '\n",
        "        return start_prompt + '\"' + dialogue + '\"' + end_prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring Script for Batch Processing\n",
        "- Develops a scoring script capable of handling batch input, transforming JSON data into DataFrames for model prediction, and outputting results in a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ../src/score_batches.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/score_batches.py\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import mlflow.pyfunc\n",
        "import os\n",
        "\n",
        "# Path where the MLflow model is saved\n",
        "model_path = \"models:/SentimentAnalysisModel/Production\"\n",
        "\n",
        "# Global variable for holding the model\n",
        "model = None\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # Load the MLflow model into the global variable\n",
        "    model = mlflow.pyfunc.load_model(model_path)\n",
        "\n",
        "def run(mini_batch):\n",
        "    # mini_batch is a list of file paths\n",
        "    results = []\n",
        "    for file_path in mini_batch:\n",
        "        # Assuming each file contains data in JSON format\n",
        "        with open(file_path) as f:\n",
        "            data = json.load(f)\n",
        "            # Convert data into DataFrame\n",
        "            df = pd.DataFrame(data[\"input_data\"][\"data\"], columns=data[\"input_data\"][\"columns\"])\n",
        "            # Perform prediction\n",
        "            prediction = model.predict(df)\n",
        "            # Assuming prediction is returned as a list of sentiments\n",
        "            result = {\"sentiment\": prediction.tolist()}\n",
        "            results.append(result)\n",
        "    \n",
        "    # Save the results to a file\n",
        "    output_file_path = os.path.join(os.getenv(\"AZUREML_BI_OUTPUT_PATH\"), \"results.json\")\n",
        "    with open(output_file_path, \"w\") as output_file:\n",
        "        json.dump(results, output_file)\n",
        "    \n",
        "    return output_file_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Endpoint and Deployment\n",
        "- Configures and deploys a batch endpoint in Azure ML, defining the model, code configuration, compute resources, and batching settings. This setup allows for efficient processing of large volumes of data, making it suitable for scenarios where real-time inference is not required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment, Model, CodeConfiguration\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Authenticate to Azure\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)\n",
        "\n",
        "# Define the model\n",
        "model_name = \"google_flan-t5-base\"\n",
        "models = ml_client.models.list(name=model_name)\n",
        "latest_model = max(models, key=lambda m: m.version)\n",
        "\n",
        "# Define the code configuration\n",
        "code_configuration = CodeConfiguration(\n",
        "    code=\"./src\",  \n",
        "    scoring_script=\"score_batches.py\"\n",
        ")\n",
        "\n",
        "# Create or update the batch endpoint\n",
        "batch_endpoint = BatchEndpoint(\n",
        "    name=\"batch-endpoint\",\n",
        "    description=\"Batch endpoint for sentiment analysis\",\n",
        "    tags={\"model\": \"SentimentAnalysisModel\"}\n",
        ")\n",
        "batch_endpoint = ml_client.begin_create_or_update(batch_endpoint)\n",
        "\n",
        "# Create the batch deployment\n",
        "batch_deployment = BatchDeployment(\n",
        "    name=\"batch-deployment\",\n",
        "    endpoint_name=batch_endpoint.name,\n",
        "    model=latest_model.id,\n",
        "    code_configuration=code_configuration,\n",
        "    compute=\"compute-cluster\",  # AMLCluster - not created due to azure trial limitations\n",
        "    mini_batch_size=\"10\",  # Adjust based on your needs\n",
        "    error_threshold=10,\n",
        "    retry_settings={\"max_retries\": 3, \"timeout\": 30}\n",
        ")\n",
        "\n",
        "# Deploy\n",
        "ml_client.begin_create_or_update(batch_deployment)\n"
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
