{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: datasets==2.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.17.0)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (1.25.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (12.0.1)\nRequirement already satisfied: pyarrow-hotfix in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (4.65.0)\nRequirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\nRequirement already satisfied: multiprocess in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.20.3)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (6.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pip in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1708920967653
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = '../src'\n",
        "output_folder = '../output'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "../src folder created\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1708921016843
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/sentiment_prediction.py\n",
        "import mlflow\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def predict_sentiment(dialogue, output_path, model_name='google/flan-t5-base'):\n",
        "    # Initialize tokeniazer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Constructing a 5-shot prompt with examples\n",
        "    start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "    Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "    Sentiment: Neutral\n",
        "\n",
        "    Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: '''\n",
        "    \n",
        "    end_prompt = '\\nSentiment: '\n",
        "    \n",
        "    # Construct the full prompt with the user-provided dialogue\n",
        "    prompt = start_prompt + '\"' + dialogue + '\"' + end_prompt \n",
        "\n",
        "    # Tokenize input dialogue\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate prediction\n",
        "    output = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "    \n",
        "    # Decode and print the prediction\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Write the predicted sentiment to the specified output file\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write('Text: ' + dialogue + '\\nPredicted Sentiment: ' + decoded_output + '\\n')\n",
        "\n",
        "def main():\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description=\"Predict sentiment from input dialogue\")\n",
        "    parser.add_argument(\"--dialogue\", type=str, required=True, help=\"Input dialogue for sentiment prediction\")\n",
        "    parser.add_argument(\"--output\", type=str, required=True, help=\"Output file path for sentiment prediction\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Predict sentiment and write to output\n",
        "    predict_sentiment(args.dialogue, args.output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/sentiment_prediction.py\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python $script_folder/sentiment_prediction.py --dialogue \"I love this book!\" --output $output_folder\"/output.txt\"\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024/02/24 06:28:48 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n2024-02-24 06:28:48.480926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-02-24 06:28:52.141555: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-02-24 06:28:53.196121: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-02-24 06:28:53.196169: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-02-24 06:29:01.806885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-02-24 06:29:01.807156: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-02-24 06:29:01.807178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2024/02/24 06:29:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n2024/02/24 06:29:12 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n2024/02/24 06:29:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\nDownloading: 100%|█████████████████████████| 2.48k/2.48k [00:00<00:00, 12.4MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 1.97MB/s]\nDownloading: 100%|█████████████████████████| 2.31M/2.31M [00:00<00:00, 12.0MB/s]\nDownloading: 100%|█████████████████████████| 2.15k/2.15k [00:00<00:00, 12.9MB/s]\nDownloading: 100%|█████████████████████████| 1.37k/1.37k [00:00<00:00, 8.36MB/s]\nDownloading: 100%|███████████████████████████| 945M/945M [00:28<00:00, 34.3MB/s]\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/dialogue_summarization_peft.py\n",
        "import mlflow\n",
        "import argparse\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "import torch\n",
        "import evaluate\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "def tokenize_and_prepare_data(dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Tokenizes the dataset and prepares it for training and evaluation.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize_function(example):\n",
        "        start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "        end_prompt = '\\n\\nSummary: '\n",
        "        prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "        example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "        example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "        return example\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
        "    tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
        "\n",
        "    return tokenized_datasets\n",
        "\n",
        "def fine_tune_with_peft(tokenized_datasets, model_name, output_dir, num_train_epochs, train_batch_size, eval_batch_size, learning_rate, lora_r, lora_alpha, lora_dropout):\n",
        "    \"\"\"\n",
        "    Fine-tunes the model using PEFT on the tokenized dataset.\n",
        "    \"\"\"\n",
        "    original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"q\", \"v\"],\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM\n",
        "    )\n",
        "    peft_model = get_peft_model(original_model, lora_config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=learning_rate,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        per_device_eval_batch_size=eval_batch_size,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Log training parameters\n",
        "    training_params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"lora_r\": lora_r,\n",
        "        \"lora_alpha\": lora_alpha,\n",
        "        \"lora_dropout\": lora_dropout,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"eval_batch_size\": eval_batch_size\n",
        "    }\n",
        "    mlflow.log_params(training_params)\n",
        "\n",
        "def evaluate_model(dataset_name, model_name, num_samples=10):\n",
        "    \"\"\"\n",
        "    Evaluates the model using a subset of the dataset and prints ROUGE scores.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    dialogues = dataset['test'][:num_samples]['dialogue']\n",
        "    human_baseline_summaries = dataset['test'][:num_samples]['summary']\n",
        "    model_summaries = []\n",
        "\n",
        "    for dialogue in dialogues:\n",
        "        prompt = f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary: \"\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        model_output = model.generate(input_ids=input_ids, max_new_tokens=200)\n",
        "        model_summary = tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
        "        model_summaries.append(model_summary)\n",
        "\n",
        "    rouge = evaluate.load('rouge')\n",
        "    results = rouge.compute(predictions=model_summaries, references=human_baseline_summaries, use_stemmer=True)\n",
        "\n",
        "    print('MODEL ROUGE SCORES:')\n",
        "    print(results)\n",
        "\n",
        "    # Log evaluation parameters\n",
        "    evaluation_params = {\n",
        "        \"evaluation_num_samples\": num_samples\n",
        "    }\n",
        "    mlflow.log_params(evaluation_params)\n",
        "\n",
        "def main():\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune and evaluate a dialogue summarization model with PEFT\")\n",
        "    parser.add_argument(\"--dataset_name\", type=str, default='knkarthick/dialogsum', help=\"Dataset name to use for training and evaluation\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default='google/flan-t5-base', help=\"Model name or path\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default='./peft_model', help=\"Output directory for saving the model\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate for training\")\n",
        "    parser.add_argument(\"--lora_r\", type=int, default=32, help=\"Rank of LoRA\")\n",
        "    parser.add_argument(\"--lora_alpha\", type=int, default=32, help=\"Scale parameter for LoRA\")\n",
        "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05, help=\"Dropout rate for LoRA layers\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Training batch size\")\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=4, help=\"Evaluation batch size\")\n",
        "    parser.add_argument(\"--num_samples\", type=int, default=10, help=\"Number of samples to use for evaluation\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    tokenized_datasets = tokenize_and_prepare_data(args.dataset_name, args.model_name)\n",
        "    fine_tune_with_peft(tokenized_datasets, args.model_name, args.output_dir, args.num_train_epochs, args.train_batch_size, args.eval_batch_size, args.learning_rate, args.lora_r, args.lora_alpha, args.lora_dropout)\n",
        "    evaluate_model(args.dataset_name, args.model_name, args.num_samples)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/dialogue_summarization_peft.py\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import ComputeInstance\n",
        "\n",
        "\n",
        "ci = ComputeInstance(\n",
        "    name=\"compute-instance\", \n",
        "    size=\"Standard_E4ds_v4\"\n",
        ")\n",
        "ml_client.begin_create_or_update(ci).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../sentiment_prediction.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: sentiment_prediction_merged\n",
        "display_name: Sentiment Prediction with Integrated Tokenization\n",
        "version: 3\n",
        "type: command\n",
        "inputs:\n",
        "  dialogue: \n",
        "    type: string\n",
        "outputs:\n",
        "  sentiment_output:\n",
        "    type: uri_file\n",
        "code: ./src\n",
        "environment: azureml:AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu@latest\n",
        "compute: azureml:cpu-cluster\n",
        "command: >-\n",
        "  python sentiment_prediction.py \n",
        "  --dialogue ${{inputs.dialogue}}\n",
        "  --output ${{outputs.sentiment_output}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../sentiment_prediction.yml\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../dialogue_summarization_peft.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: dialogue_summarization_peft\n",
        "display_name: Dialogue Summarization with PEFT\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  dataset_name: \n",
        "    type: string\n",
        "  model_name:\n",
        "    type: string\n",
        "    default: google/flan-t5-base\n",
        "  output_dir:\n",
        "    type: string\n",
        "    default: ./peft_model\n",
        "  learning_rate:\n",
        "    type: number\n",
        "    default: 0.001\n",
        "  lora_r:\n",
        "    type: integer\n",
        "    default: 32\n",
        "  lora_alpha:\n",
        "    type: integer\n",
        "    default: 32\n",
        "  lora_dropout:\n",
        "    type: number\n",
        "    default: 0.05\n",
        "  num_train_epochs:\n",
        "    type: integer\n",
        "    default: 1\n",
        "  train_batch_size:\n",
        "    type: integer\n",
        "    default: 4\n",
        "  eval_batch_size:\n",
        "    type: integer\n",
        "    default: 4\n",
        "  num_samples:\n",
        "    type: integer\n",
        "    default: 10\n",
        "code: ./\n",
        "environment: AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu@latest\n",
        "command: >-\n",
        "  python dialogue_summarization_peft.py \n",
        "  --dataset_name ${{inputs.dataset_name}}\n",
        "  --model_name ${{inputs.model_name}}\n",
        "  --output_dir ${{inputs.output_dir}}\n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --lora_r ${{inputs.lora_r}}\n",
        "  --lora_alpha ${{inputs.lora_alpha}}\n",
        "  --lora_dropout ${{inputs.lora_dropout}}\n",
        "  --num_train_epochs ${{inputs.num_train_epochs}}\n",
        "  --train_batch_size ${{inputs.train_batch_size}}\n",
        "  --eval_batch_size ${{inputs.eval_batch_size}}\n",
        "  --num_samples ${{inputs.num_samples}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../dialogue_summarization_peft.yml\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "predict_sentiment_segment = load_component(source=parent_dir + \"../sentiment_prediction.yml\")\n",
        "\n",
        "# register component\n",
        "prep = ml_client.components.create_or_update(predict_sentiment_segment, version='2')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1708756418880
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def sentiment_prediction(pipeline_job_input):\n",
        "    sentiment = predict_sentiment_segment(dialogue=pipeline_job_input)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_predict_sentiment_data\": sentiment.outputs.sentiment_output,\n",
        "        \n",
        "    }\n",
        "\n",
        "# Example usage of the pipeline with a direct string input for the dialogue\n",
        "pipeline_job = sentiment_prediction(pipeline_job_input=\"This is an example dialogue text.\")\n",
        "pipeline_job.settings.default_compute = \"compute-instance\""
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708756451136
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"sentiment_prediction\"\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f11ac770df0>}, 'outputs': {'pipeline_job_predict_sentiment_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f11ac7703a0>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f11ac7706d0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'sentiment_prediction', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_predict_sentiment_data': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'sentiment': Command({'parameters': {}, 'init': False, 'name': 'sentiment', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f11ac770430>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'dialogue': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'sentiment_output': '${{parent.outputs.pipeline_job_predict_sentiment_data}}'}, 'inputs': {'dialogue': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f11ac7704f0>}, 'outputs': {'sentiment_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f11ac770490>}, 'component': 'azureml_anonymous:1c2ea3ea-541f-4330-84ac-c272900bdfbd', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'f95b8e93-0a1a-4e6f-809b-89fc5e007ed6', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'sharp_fennel_82r1k1c1x6', 'description': None, 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{\"pipeline_job_input\":\"This is an example dialogue text.\"}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'compute-instance', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/jobs/sharp_fennel_82r1k1c1x6', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f11ac7710f0>, 'serialize': <msrest.serialization.Serializer object at 0x7f11ac770940>, 'display_name': 'sentiment_prediction', 'experiment_name': 'sentiment_prediction', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://australiasoutheast.api.azureml.ms/mlflow/v1.0/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/sharp_fennel_82r1k1c1x6?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&tid=df64ec22-8387-4f67-874d-a8321645e4ba', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>sentiment_prediction</td><td>sharp_fennel_82r1k1c1x6</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/sharp_fennel_82r1k1c1x6?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&amp;tid=df64ec22-8387-4f67-874d-a8321645e4ba\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708756461393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #CLI2 version of creating component and pipeline\n",
        "# !az extension add --name ml -y\n",
        "# output = %sx az ml component list \\\n",
        "#         --resource-group \"cloud-shell-storage-southeastasia\" \\\n",
        "#         --workspace-name \"oksana_ml\"\n",
        "# print(output)\n",
        "# !az ml component create --file ../sentiment_prediction.yml\n",
        "# !az ml job create --file ../pipeline_sentiment_prediction.yml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "name": "ml.geospatial.interactive",
        "_defaultOrder": 20,
        "hideHardwareSpecs": true,
        "vcpuNum": 0,
        "_isFastLaunch": false,
        "gpuNum": 0,
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "category": "General purpose",
        "memoryGiB": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}