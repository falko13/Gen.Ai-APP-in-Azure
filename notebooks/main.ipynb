{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: datasets==2.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.17.0)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (1.25.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (12.0.1)\nRequirement already satisfied: pyarrow-hotfix in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (4.65.0)\nRequirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\nRequirement already satisfied: multiprocess in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (0.20.3)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.17.0) (6.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pip in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge_score==0.1.2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting rouge_score==0.1.2\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25hCollecting absl-py (from rouge_score==0.1.2)\n  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nltk (from rouge_score==0.1.2)\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.25.0)\nRequirement already satisfied: six>=1.14.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.16.0)\nRequirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (8.0.4)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (1.2.0)\nCollecting regex>=2021.8.3 (from nltk->rouge_score==0.1.2)\n  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (4.65.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a3c3d0b4b1c0dbabd3add3cc260969cc9d6428fc866bf3c58848b102d6dd7f4e\n  Stored in directory: /home/azureuser/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: regex, absl-py, nltk, rouge_score\nSuccessfully installed absl-py-2.1.0 nltk-3.8.1 regex-2023.12.25 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Bad pipe message: %s [b\"/\\xc2\\xbb\\xa3\\x1e>\\xd2\\xe3\\x1bM&a\\xd3.G\\xf80\\xad ,^\\xe8\\xed\\xa8\\xffa(.\\xe7\\x04\\x8e\\xb3\\xb5\\xa9\\x9cihK'\\xd8\\xc7c\\x8cYT\\x99\\x97\\xbc\\xc3U\\xd1\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xb8\\x01\\xcb\\x95\\x16\\x85\\xed\\x95\\xc4@\\x12\\\\\\xc8k\\xee\\xcf\\x18p\\xfd\\xfa\\xdcN\\x7f\\x9c\\xa2\\xc0\\xa2xX\\xcb\\x08\"]\nBad pipe message: %s [b\"\\xfa1Y;2\\x0e\\xd9\\xb7\\xd7\\x1b\\x91\\x19\\x8f\\xe7\\xfa$W^\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\"]\nBad pipe message: %s [b'k\\x8fw[\\xc3\\x1an\\xff\\xb64)\\xa8\\xf0G\\xb96g\\x0e\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0']\nBad pipe message: %s [b'3\\x002\\x001\\x000\\x00']\nBad pipe message: %s [b'\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00']\nBad pipe message: %s [b'\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\nBad pipe message: %s [b'\\x1c\\xc7V\\xca*\\x93=\\xc0v\\xf3z\\x11\\x00\\xae\\xd9#\\xdbp\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*', b\"\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\\x03\\x03\\x02\\x01\\x02\\x02\"]\nBad pipe message: %s [b\"l\\xad\\xa9\\xb4\\x12\\xd4\\xe5\\xa9/\\x0f\\x08gp\\x13\\x99s\\xfdF\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\", b'\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07']\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709001370562
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1709013016639
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = '../src'\n",
        "output_folder = '../output'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "../src folder created\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1709013017124
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/sentiment_prediction.py\n",
        "import mlflow\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def predict_sentiment(dialogue, output_path, model_name='google/flan-t5-base'):\n",
        "    # Initialize tokeniazer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Constructing a 5-shot prompt with examples\n",
        "    start_prompt = '''Provide Sentiment for the following comment/conversation (possible sentiments: Positive, Negative, Neutral):\n",
        "\n",
        "    Comment: \"I love sunny days, they make me feel so happy!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"This is the worst experience of my life, I'm so disappointed.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: \"I'm not sure how I feel about this new policy. It might be good or bad.\"\n",
        "    Sentiment: Neutral\n",
        "\n",
        "    Comment: \"The service at this restaurant was fantastic, best dinner ever!\"\n",
        "    Sentiment: Positive\n",
        "\n",
        "    Comment: \"I waited for an hour and my order was still wrong.\"\n",
        "    Sentiment: Negative\n",
        "\n",
        "    Comment: '''\n",
        "    \n",
        "    end_prompt = '\\nSentiment: '\n",
        "    \n",
        "    # Construct the full prompt with the user-provided dialogue\n",
        "    prompt = start_prompt + '\"' + dialogue + '\"' + end_prompt \n",
        "\n",
        "    # Tokenize input dialogue\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate prediction\n",
        "    output = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "    \n",
        "    # Decode and print the prediction\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Write the predicted sentiment to the specified output file\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write('Text: ' + dialogue + '\\nPredicted Sentiment: ' + decoded_output + '\\n')\n",
        "\n",
        "def main():\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description=\"Predict sentiment from input dialogue\")\n",
        "    parser.add_argument(\"--dialogue\", type=str, required=True, help=\"Input dialogue for sentiment prediction\")\n",
        "    parser.add_argument(\"--output\", type=str, required=True, help=\"Output file path for sentiment prediction\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Predict sentiment and write to output\n",
        "    predict_sentiment(args.dialogue, args.output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/sentiment_prediction.py\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python $script_folder/sentiment_prediction.py --dialogue \"I love this book!\" --output $output_folder\"/output.txt\"\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024/02/24 06:28:48 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n2024-02-24 06:28:48.480926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-02-24 06:28:52.141555: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-02-24 06:28:53.196121: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-02-24 06:28:53.196169: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-02-24 06:29:01.806885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-02-24 06:29:01.807156: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-02-24 06:29:01.807178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2024/02/24 06:29:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n2024/02/24 06:29:12 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n2024/02/24 06:29:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\nDownloading: 100%|█████████████████████████| 2.48k/2.48k [00:00<00:00, 12.4MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 1.97MB/s]\nDownloading: 100%|█████████████████████████| 2.31M/2.31M [00:00<00:00, 12.0MB/s]\nDownloading: 100%|█████████████████████████| 2.15k/2.15k [00:00<00:00, 12.9MB/s]\nDownloading: 100%|█████████████████████████| 1.37k/1.37k [00:00<00:00, 8.36MB/s]\nDownloading: 100%|███████████████████████████| 945M/945M [00:28<00:00, 34.3MB/s]\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/dialogue_summarization_peft.py\n",
        "import mlflow\n",
        "import argparse\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "import torch\n",
        "import evaluate\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "def tokenize_and_prepare_data(dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Tokenizes the dataset and prepares it for training and evaluation.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize_function(example):\n",
        "        start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "        end_prompt = '\\n\\nSummary: '\n",
        "        prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "        example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "        example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "        return example\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
        "    tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
        "\n",
        "    return tokenized_datasets\n",
        "\n",
        "def fine_tune_with_peft(tokenized_datasets, model_name, output_dir, num_train_epochs, train_batch_size, eval_batch_size, learning_rate, lora_r, lora_alpha, lora_dropout):\n",
        "    \"\"\"\n",
        "    Fine-tunes the model using PEFT on the tokenized dataset.\n",
        "    \"\"\"\n",
        "    original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"q\", \"v\"],\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM\n",
        "    )\n",
        "    peft_model = get_peft_model(original_model, lora_config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=learning_rate,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        per_device_eval_batch_size=eval_batch_size,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Log training parameters\n",
        "    training_params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"lora_r\": lora_r,\n",
        "        \"lora_alpha\": lora_alpha,\n",
        "        \"lora_dropout\": lora_dropout,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"eval_batch_size\": eval_batch_size\n",
        "    }\n",
        "    mlflow.log_params(training_params)\n",
        "\n",
        "def evaluate_model(dataset_name, model_name, num_samples=10):\n",
        "    \"\"\"\n",
        "    Evaluates the model using a subset of the dataset and prints ROUGE scores.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    dialogues = dataset['test'][:num_samples]['dialogue']\n",
        "    human_baseline_summaries = dataset['test'][:num_samples]['summary']\n",
        "    model_summaries = []\n",
        "\n",
        "    for dialogue in dialogues:\n",
        "        prompt = f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary: \"\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        model_output = model.generate(input_ids=input_ids, max_new_tokens=200)\n",
        "        model_summary = tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
        "        model_summaries.append(model_summary)\n",
        "\n",
        "    rouge = evaluate.load('rouge')\n",
        "    results = rouge.compute(predictions=model_summaries, references=human_baseline_summaries, use_stemmer=True)\n",
        "\n",
        "    print('MODEL ROUGE SCORES:')\n",
        "    print(results)\n",
        "\n",
        "    # Log evaluation parameters\n",
        "    evaluation_params = {\n",
        "        \"evaluation_num_samples\": num_samples\n",
        "    }\n",
        "    mlflow.log_params(evaluation_params)\n",
        "\n",
        "def main():\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune and evaluate a dialogue summarization model with PEFT\")\n",
        "    parser.add_argument(\"--dataset_name\", type=str, default='knkarthick/dialogsum', help=\"Dataset name to use for training and evaluation\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default='google/flan-t5-base', help=\"Model name or path\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default='./peft_model', help=\"Output directory for saving the model\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate for training\")\n",
        "    parser.add_argument(\"--lora_r\", type=int, default=32, help=\"Rank of LoRA\")\n",
        "    parser.add_argument(\"--lora_alpha\", type=int, default=32, help=\"Scale parameter for LoRA\")\n",
        "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05, help=\"Dropout rate for LoRA layers\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Training batch size\")\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=4, help=\"Evaluation batch size\")\n",
        "    parser.add_argument(\"--num_samples\", type=int, default=10, help=\"Number of samples to use for evaluation\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    tokenized_datasets = tokenize_and_prepare_data(args.dataset_name, args.model_name)\n",
        "    fine_tune_with_peft(tokenized_datasets, args.model_name, args.output_dir, args.num_train_epochs, args.train_batch_size, args.eval_batch_size, args.learning_rate, args.lora_r, args.lora_alpha, args.lora_dropout)\n",
        "    evaluate_model(args.dataset_name, args.model_name, args.num_samples)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/dialogue_summarization_peft.py\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import ComputeInstance\n",
        "\n",
        "\n",
        "ci = ComputeInstance(\n",
        "    name=\"compute-instance\", \n",
        "    size=\"Standard_E4ds_v4\"\n",
        ")\n",
        "ml_client.begin_create_or_update(ci).result()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../src/conda-env.yml\n",
        "name: basic-env-cpu\n",
        "channels:\n",
        "  - conda-forge\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - pip\n",
        "  - pip:\n",
        "    - torch==1.13.1\n",
        "    - rouge_score==0.1.2\n",
        "  - python=3.10.11\n",
        "  - datasets==2.17.0\n",
        "  - torchdata==0.5.1\n",
        "  - transformers==4.27.2\n",
        "  - evaluate==0.4.0\n",
        "  - loralib==0.1.1\n",
        "  - peft==0.3.0\n",
        "  - mlflow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../src/conda-env.yml\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708996265603
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls ../src"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[0m\u001b[01;32mconda-env.yml\u001b[0m*  \u001b[01;32mdialogue_summarization_peft.py\u001b[0m*  \u001b[01;32msentiment_prediction.py\u001b[0m*\r\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709004070187
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create new environment using base Docker image and conda specs.\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:latest\",\n",
        "    conda_file=\"../src/conda-env.yml\",\n",
        "    name=\"docker-image-llm\",\n",
        "    description=\"llm from docker\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'docker-image-llm', 'description': 'llm from docker', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/environments/docker-image-llm/versions/9', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f4ca18bd5d0>, 'serialize': <msrest.serialization.Serializer object at 0x7f4ca18bd150>, 'version': '9', 'latest_version': None, 'conda_file': {'channels': ['conda-forge', 'defaults'], 'dependencies': ['pip', {'pip': ['torch==1.13.1', 'rouge_score==0.1.2']}, 'python=3.10.11', 'datasets==2.17.0', 'torchdata==0.5.1', 'transformers==4.27.2', 'evaluate==0.4.0', 'loralib==0.1.1', 'peft==0.3.0', 'mlflow'], 'name': 'basic-env-cpu'}, 'image': 'mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:latest', 'build': None, 'inference_config': None, 'os_type': 'Linux', 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"conda-forge\",\\n    \"defaults\"\\n  ],\\n  \"dependencies\": [\\n    \"pip\",\\n    {\\n      \"pip\": [\\n        \"torch==1.13.1\",\\n        \"rouge_score==0.1.2\"\\n      ]\\n    },\\n    \"python=3.10.11\",\\n    \"datasets==2.17.0\",\\n    \"torchdata==0.5.1\",\\n    \"transformers==4.27.2\",\\n    \"evaluate==0.4.0\",\\n    \"loralib==0.1.1\",\\n    \"peft==0.3.0\",\\n    \"mlflow\"\\n  ],\\n  \"name\": \"basic-env-cpu\"\\n}'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709009537357
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../sentiment_prediction.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: sentiment_prediction_merged\n",
        "display_name: Sentiment Prediction with Integrated Tokenization\n",
        "version: 3\n",
        "type: command\n",
        "inputs:\n",
        "  dialogue: \n",
        "    type: string\n",
        "outputs:\n",
        "  sentiment_output:\n",
        "    type: uri_file\n",
        "code: ./src\n",
        "environment: azureml:docker-image-llm@latest\n",
        "compute: azureml:cpu-cluster\n",
        "command: >-\n",
        "  python sentiment_prediction.py \n",
        "  --dialogue ${{inputs.dialogue}}\n",
        "  --output ${{outputs.sentiment_output}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../sentiment_prediction.yml\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../dialogue_summarization_peft.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: dialogue_summarization_peft\n",
        "display_name: Dialogue Summarization with PEFT\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  dataset_name: \n",
        "    type: string\n",
        "  model_name:\n",
        "    type: string\n",
        "    default: google/flan-t5-base\n",
        "  output_dir:\n",
        "    type: string\n",
        "    default: ./peft_model\n",
        "  learning_rate:\n",
        "    type: number\n",
        "    default: 0.001\n",
        "  lora_r:\n",
        "    type: integer\n",
        "    default: 32\n",
        "  lora_alpha:\n",
        "    type: integer\n",
        "    default: 32\n",
        "  lora_dropout:\n",
        "    type: number\n",
        "    default: 0.05\n",
        "  num_train_epochs:\n",
        "    type: integer\n",
        "    default: 1\n",
        "  train_batch_size:\n",
        "    type: integer\n",
        "    default: 4\n",
        "  eval_batch_size:\n",
        "    type: integer\n",
        "    default: 4\n",
        "  num_samples:\n",
        "    type: integer\n",
        "    default: 10\n",
        "code: ./\n",
        "environment: azureml:docker-image-llm@latest\n",
        "command: >-\n",
        "  python dialogue_summarization_peft.py \n",
        "  --dataset_name ${{inputs.dataset_name}}\n",
        "  --model_name ${{inputs.model_name}}\n",
        "  --output_dir ${{inputs.output_dir}}\n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --lora_r ${{inputs.lora_r}}\n",
        "  --lora_alpha ${{inputs.lora_alpha}}\n",
        "  --lora_dropout ${{inputs.lora_dropout}}\n",
        "  --num_train_epochs ${{inputs.num_train_epochs}}\n",
        "  --train_batch_size ${{inputs.train_batch_size}}\n",
        "  --eval_batch_size ${{inputs.eval_batch_size}}\n",
        "  --num_samples ${{inputs.num_samples}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ../dialogue_summarization_peft.yml\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "predict_sentiment_segment = load_component(source=parent_dir + \"../sentiment_prediction.yml\")\n",
        "\n",
        "# register component\n",
        "prep = ml_client.components.create_or_update(predict_sentiment_segment, version='6')"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1709015832518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def sentiment_prediction(pipeline_job_input):\n",
        "    sentiment = predict_sentiment_segment(dialogue=pipeline_job_input)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_predict_sentiment_data\": sentiment.outputs.sentiment_output,\n",
        "        \n",
        "    }\n",
        "\n",
        "# Example usage with a direct string input for the dialogue\n",
        "pipeline_job = sentiment_prediction(pipeline_job_input='\"Movie is rathet bad!\"')\n",
        "pipeline_job.settings.default_compute = \"compute-instance\""
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709015835758
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"sentiment_prediction\"\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f300ffbf5b0>}, 'outputs': {'pipeline_job_predict_sentiment_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f300ffbf580>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f300ffbf4f0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'sentiment_prediction', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_predict_sentiment_data': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'sentiment': Command({'parameters': {}, 'init': False, 'name': 'sentiment', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f300ff25240>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'dialogue': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'sentiment_output': '${{parent.outputs.pipeline_job_predict_sentiment_data}}'}, 'inputs': {'dialogue': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f300ffbf760>}, 'outputs': {'sentiment_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f301c116260>}, 'component': 'azureml_anonymous:2d227dc3-faa8-4948-bcf6-19c46f4b92ab', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '115284f9-9688-4eec-88f1-bd80390b9a93', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'busy_chicken_qw3fty2g06', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/falko13/Gen.Ai-APP-in-Azure.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'c8134dbc4eadd7865a07d95bc43e9914f7c8c1f8', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{\"pipeline_job_input\":\"\\\\\"Movie is rathet bad!\\\\\"\"}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'compute-instance', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml/jobs/busy_chicken_qw3fty2g06', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance/code/Users/opanasenko2084/Gen.Ai-APP-in-Azure/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f300ffbf550>, 'serialize': <msrest.serialization.Serializer object at 0x7f300ffbf220>, 'display_name': 'sentiment_prediction', 'experiment_name': 'sentiment_prediction', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://australiasoutheast.api.azureml.ms/mlflow/v1.0/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourceGroups/cloud-shell-storage-southeastasia/providers/Microsoft.MachineLearningServices/workspaces/oksana_ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/busy_chicken_qw3fty2g06?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&tid=df64ec22-8387-4f67-874d-a8321645e4ba', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>sentiment_prediction</td><td>busy_chicken_qw3fty2g06</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/busy_chicken_qw3fty2g06?wsid=/subscriptions/71dd2dd9-4027-4b07-a6aa-e98b8b31e8cc/resourcegroups/cloud-shell-storage-southeastasia/workspaces/oksana_ml&amp;tid=df64ec22-8387-4f67-874d-a8321645e4ba\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709015843005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for creating components, pipelines, and submitting jobs for LLM text summarization model training.\n",
        "from azure.ai.ml import load_component, Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "parent_dir = \"\"\n",
        "\n",
        "# Load the dialogue summarization component from a YAML file\n",
        "dialogue_summarization_peft_component = load_component(source=parent_dir + \"../dialogue_summarization_peft.yml\")\n",
        "\n",
        "# Register the component with a specific version\n",
        "registered_component = ml_client.components.create_or_update(dialogue_summarization_peft_component, version='1')\n",
        "\n",
        "# Define a pipeline function for training the dialogue summarization model\n",
        "@pipeline(description=\"Pipeline for training dialogue summarization model with PEFT\")\n",
        "def dialogue_summarization_pipeline_peft():\n",
        "    # Use the registered component within the pipeline\n",
        "    summarization_task = registered_component()\n",
        "\n",
        "\n",
        "# Instantiate the pipeline\n",
        "pipeline_job = dialogue_summarization_pipeline_peft()\n",
        "\n",
        "# Set the default compute target for the pipeline job\n",
        "pipeline_job.settings.default_compute = \"your-compute-instance\"\n",
        "\n",
        "# Submit the pipeline job to the workspace\n",
        "submitted_job = ml_client.jobs.create_or_update(pipeline_job, experiment_name=\"Dialogue_Summarization_Experiment\")\n",
        "\n",
        "# Output the submitted job details\n",
        "print(submitted_job)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #CLI2 version of creating component and pipeline\n",
        "# !az extension add --name ml -y\n",
        "# output = %sx az ml component list \\\n",
        "#         --resource-group \"cloud-shell-storage-southeastasia\" \\\n",
        "#         --workspace-name \"oksana_ml\"\n",
        "# print(output)\n",
        "# !az ml component create --file ../sentiment_prediction.yml\n",
        "# !az ml job create --file ../pipeline_sentiment_prediction.yml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "name": "ml.geospatial.interactive",
        "_defaultOrder": 20,
        "hideHardwareSpecs": true,
        "vcpuNum": 0,
        "_isFastLaunch": false,
        "gpuNum": 0,
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "category": "General purpose",
        "memoryGiB": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}